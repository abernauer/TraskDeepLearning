---
title: 'Chapter 5: Learning mulitple weights at a time:'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Gradient descent learning with multiple inputs

*An empty network with multiple inputs*
```{python}
def w_sum(a,b):
    assert(len(a) == len(b))
    
    output = 0 
    
    for i in range(len(a)) :
        output += (a[i] * b[i])
        
    return output
    
weights = [0.1 , 0.2, -.1]

def neural_net(input, weights):

    pred = w_sum(input, weights)
    
    return pred
```
  
*Predict and Compare*

```{python}
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true = win_or_lose_binary[0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weights)

error = (pred - true) ** 2

delta = pred - true
```


```{r}
```

  
  
  
  
  

# Gradient descent multiple inputs explained 

# Watching several steps of learning 


# Freezing one weight:


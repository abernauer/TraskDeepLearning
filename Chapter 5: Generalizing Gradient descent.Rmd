---
title: 'Chapter 5: Learning mulitple weights at a time:'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Gradient descent learning with multiple inputs

**An empty network with multiple inputs**
```{python}
def w_sum(a,b):
    assert(len(a) == len(b))
    
    output = 0 
    
    for i in range(len(a)) :
        output += (a[i] * b[i])
        
    return output
    
weights = [0.1 , 0.2, -.1]

def neural_net(input, weights):

    pred = w_sum(input, weights)
    
    return pred
```
  
**Predict and Compare**

```{python}
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weights)

error = (pred - true_pred) ** 2

delta = pred - true
```

**Learn calc each weight_delta**
```{python}
def ele_mul(number, vector):
    
    output = [0, 0, 0]
    
    assert(len(output) == len(vector))
    
    for i in range(len(vector)):
        output[i] = number * vector[i]
    return output

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weight)

error = (pred - true_pred) ** 2

delta = pred - true_pred

weight_deltas = ele_mul(delta, input)
```

**Learn updating the weights **  
```{python}
input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weight)
error = (pred - true_pred) ** 2
delta = pred - true_pred

weight_deltas = ele_mul(delta, input)

for i in range(len(weights)):
    weights[i] -= alpha * weight_deltas[i]
print("Weights: "  + str(weights))
print("Weights Deltas:" + str(weight_deltas))
```
  
  
  
  
  

# Gradient descent multiple inputs explained 
Single input: prediction and calcing error and delta

```{python}
number_of_toes =[8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]
true = win_or_lose_binary[0]
pred = neural_net(input, weight)

error = (pred - true_pred) ** 2

delta = pred - true
```

Multiple inputs: prediction and calcing error and delta
```{python}
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weights)

error = (pred - true_pred) ** 2

delta = pred - true_pred
```


Single input 
```{python}
number_of_toes =[8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]
true = win_or_lose_binary[0]
pred = neural_net(input, weight)

error = (pred - true_pred) ** 2

delta = pred - true

weight_delta = input * delta
```


Multi input
```{python}
def ele_mul(number, vector):
    
    output = [0, 0, 0]
    
    assert(len(output) == len(vector))
    
    for i in range(len(vector)):
        output[i] = number * vector[i]
    return output

toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weights)

error = (pred - true_pred) ** 2

delta = pred - true_pred

weight_deltas = ele_mul(delta, input)


```

updating the weight 
```{python}
number_of_toes =[8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]
true = win_or_lose_binary[0]
pred = neural_net(input, weight)

error = (pred - true_pred) ** 2

delta = pred - true

weight_delta = input * delta

alpha = 0.01 

weight -= weight_delta * alpha
```


updating the weights 

```{python}
def ele_mul(number, vector):
    
    output = [0, 0, 0]
    
    assert(len(output) == len(vector))
    
    for i in range(len(vector)):
        output[i] = number * vector[i]
    return output

toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_net(input, weights)

error = (pred - true_pred) ** 2

delta = pred - true_pred

weight_deltas = ele_mul(delta, input)

alpha = 0.01 

for i in range(len(weights)):
    weights[i] -= alpha * weight_deltas[i]
```



# Watching several steps of learning 
```{python}
def neural_net(input, weights) :
  out = 0
  for i in range(len(input)):
      out += (input[i] * weights[i])
  return out

def ele_mul(scalar, vector):
  out = [0, 0, 0]
  for i in range(len(out)):
      out[i] = vector[i] * scalar
  return out
  
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

alpha = 0.01
weigths = [0.1, 0.2, -.1]

input = [toes[0], wlrec[0], nfans[0]]

for iter in range(3):
    pred = neural_net(input, weights)
    
    error = (pred - true_pred) ** 2
    delta = pred - true_pred
    
    weight_deltas = ele_mul(delta, input)
    
    print("Iteration:" + str(iter+1))
    print("Pred:" + str(pred))
    print("Error:" + str(error))
    print("Delta:" + str(delta))
    print("Weight_Deltas")
    print(str(weight_deltas)
    
    )
    for i in range(len(weights)):
       weights[i]-= alpha * weight_deltas[i]

```

Because the first input on our network is vary large it has a high influence on our derivative and learning too. Additionally, this causes us to alpha the learning rate to a low value of 0.01 to avoid divergence. 

```{python}
def neural_net(input, weights) :
  out = 0
  for i in range(len(input)):
      out += (input[i] * weights[i])
  return out

def ele_mul(scalar, vector):
  out = [0, 0, 0]
  for i in range(len(out)):
      out[i] = vector[i] * scalar
  return out
  
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

alpha = 0.1
weigths = [0.1, 0.2, -.1]

input = [toes[0], wlrec[0], nfans[0]]

for iter in range(3):
    pred = neural_net(input, weights)
    
    error = (pred - true_pred) ** 2
    delta = pred - true_pred
    
    weight_deltas = ele_mul(delta, input)
   
    
    print("Iteration:" + str(iter+1))
    print("Pred:" + str(pred))
    print("Error:" + str(error))
    print("Delta:" + str(delta))
    print("Weight_Deltas")
    print(str(weight_deltas)
    
    )
    for i in range(len(weights)):
       weights[i]-= alpha * weight_deltas[i]

```

Above setting alpha to 0.1 causes our prediction to jump around our goal prediction of one and beyond the goal prediction. 


# Freezing one weight:


```{python}
def neural_net(input, weights) :
  out = 0
  for i in range(len(input)):
      out += (input[i] * weights[i])
  return out

def ele_mul(scalar, vector):
  out = [0, 0, 0]
  for i in range(len(out)):
      out[i] = vector[i] * scalar
  return out
  
toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

win_or_lose_binary = [1, 1, 0, 1]

true_pred = win_or_lose_binary[0]

alpha = 0.3
weigths = [0.1, 0.2, -.1]

input = [toes[0], wlrec[0], nfans[0]]

for iter in range(3):
    pred = neural_net(input, weights)
    
    error = (pred - true_pred) ** 2
    delta = pred - true_pred
    
    weight_deltas = ele_mul(delta, input)
    weight_deltas[0] = 0
    
    print("Iteration:" + str(iter+1))
    print("Pred:" + str(pred))
    print("Error:" + str(error))
    print("Delta:" + str(delta))
    print("Weight_Deltas")
    print(str(weight_deltas)
    
    )
    for i in range(len(weights)):
       weights[i]-= alpha * weight_deltas[i]

```

Reminder to add brief description that we are fixing the error curve at zero in one of four dimensions on a plane. 


# Gradient descent learning with multiple outputs


**Empty net of multiple inputs **

```{python}
weights = [0.3, 0.2, 0.9]

def ele_mul(scalar, vector):
    out = [0, 0, 0]
    for i in range(len(out)):
        out[i] = vector[i] * scalar
    return out

def neural_net(input, weights):

    pred = ele_mul(input, weights)
    return pred

```


**Predict and calculating error and delta**

```{python}
wlrec = [0.65, 1.0, 1.0, 0.9]

hurt = [0.1, 0.0, 0.0, 0.1]
win = [ 1, 1, 0, 1]
sad = [0.1, 0.0, 0.1, 0.2]

input = wlrec[0]

true_pred = [hurt[0], win[0], sad[0]]

pred = neural_net(input, weights)

error = [0, 0, 0]
delta = [0, 0, 0]

for i in range(len(true_pred)): 
    
    error[i] = (pred[i] - true_pred[i]) ** 2
    delta[i] = pred[i] - true_pred[i]
```





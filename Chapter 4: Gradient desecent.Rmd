    ---
title: 'Chapter 4: Gradient Descent'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

  
# Predict, compare, learn 

 Questions asked in the proccess of reading chapter 3, "How do we learn we set weights so our Network's predictions are accurate?" That is the main focus of this chapter. 

1. Make Predictions
2. Compare prediction vs actual 
3. Learn
 
# Compare 

The next step in the proccess is to 

```{python}
knob_weight = 0.5
input = 0.5
goal_pred = 0.8

pred = input * knob_weight

error = (pred - goal_pred) ** 2

print(error)

```





# Learn





# Why measure error?


# Hot and Cold Learning 


```{python}
# empty net start
weight = 0.1

lr = 0.01

def neural_network(input, weight) :
    
    prediction = input * weight
    return prediction
# empty net end
# predict start
number_of_toes = [8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]

true =  win_or_lose_binary[0] 
pred = neural_network(input, weight)

error = (pred - true) ** 2
print(error)
# predict end

p_up = neural_network(input, weight+lr)
e_up = (p_up - true) ** 2
print(e_up)

p_dn = neural_network(input, weight-lr)
e_dn = (p_dn - true) ** 2
print(e_dn)

if (error  > e_dn  or error > e_up):
  
  if(e_dn < e_up):
      weight -= lr
      
  if(e_dn > e_up):
      weight += lr

    
```




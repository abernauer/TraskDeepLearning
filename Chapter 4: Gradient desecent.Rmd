    ---
title: 'Chapter 4: Gradient Descent'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

  
# Predict, compare, learn 

 Questions asked in the proccess of reading chapter 3, "How do we learn we set weights so our Network's predictions are accurate?" That is the main focus of this chapter. 

1. Make Predictions
2. Compare prediction vs actual 
3. Learn
 
# Compare 

The next step in the proccess is to 

```{python}
knob_weight = 0.5
input = 0.5
goal_pred = 0.8

pred = input * knob_weight

error = (pred - goal_pred) ** 2

print(error)

```





# Learn





# Why measure error?


# Hot and Cold Learning 


```{python}
# empty net start
weight = 0.1

lr = 0.01

def neural_network(input, weight) :
    
    prediction = input * weight
    return prediction
# empty net end
# predict start
number_of_toes = [8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]

true =  win_or_lose_binary[0] 
pred = neural_network(input, weight)

error = (pred - true) ** 2
print(error)
# predict end

p_up = neural_network(input, weight+lr)
e_up = (p_up - true) ** 2
print(e_up)

p_dn = neural_network(input, weight-lr)
e_dn = (p_dn - true) ** 2
print(e_dn)

if (error  > e_dn  or error > e_up):
  
  if(e_dn < e_up):
      weight -= lr
      
  if(e_dn > e_up):
      weight += lr

    
```

```{python}
weight = 0.5
input = 0.5
goal_pred = 0.8

step_amt = 0.001

for iteration in range(1101):

    prediction = input * weight
    error = (prediction - goal_pred) ** 2
    
    print("Error:" + str(error) + " Prediction: " + str(prediction))
    
    up_prediction = input * (weight + step_amt)
    up_error = (goal_pred - up_prediction) ** 2
    
    down_prediction = input * (weight - step_amt)
    down_error = (goal_pred - down_prediction) ** 2
    
    if(down_error < up_error):
        weight = weight - step_amt
        
    if(down_error > up_error):
        weight = weight + step_amt

```

# Calculating both direction and amount from error

_gradient descent_
```{r}
DiagrammeR::grViz(" digraph grad_descent {
graph [style = dot, rankdir = LR]

node [shape = circle]

'0.5'
'0.4'
'0.30'

'0.5' -> '0.4' [label = '-0.2']
}
                  ")
```




```{python}
weight = 0.5
goal_pred = 0.8
input = 0.5 

for iteration in range(20):
    pred = input * weight
    error = (pred - goal_pred) ** 2
    direction_and_amount = (pred - goal_pred) * input # gradient descent step
    weight = weight - direction_and_amount
    
    print("Error:" + str(error) + " Predictions:" + str(pred))
```


# One iteration of Gradient descent

```{r}
DiagrammeR::grViz("digraph empty_net{
graph [style = dot, rankdir = LR]

node [shape = circle]

'# toes' [xlabel = 'input data goes here']
'win?' [xlabel = 'Predictions come out here']

'# toes' -> 'win?' [label = '0.1']
}")
```

```{python}
weight = 0.1 
alpha = 0.01 

def neural_network(input, weight):
    prediction = input * weight
    return prediction

```


```{r}
weight <- 0.1
alpha <- 0.01

neural_net <- function (input, weight)
{
  prediction <- input * weight
  cat(prediction)
}
```


```{r}
DiagrammeR::grViz("digraph predict{
graph [style = neato, rankdir = LR]

node [shape = circle]
'0.85'
'8.5' 
'0.023' [xlabel = 'Error', style = dashed]

'8.5' -> '0.85' [label = '0.1']
}")
```


```{python}
weight = 0.1 
alpha = 0.01 

def neural_network(input, weight):
    prediction = input * weight
    return prediction
    
number_of_toes = [8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input, weight)

error = (pred - goal_pred) ** 2

```


*Compare*
```{python}
weight = 0.1 
alpha = 0.01 

def neural_network(input, weight):
    prediction = input * weight
    return prediction
    
number_of_toes = [8.5]
win_or_lose_binary = [1] # (won !!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input, weight)

error = (pred - goal_pred) ** 2

delta = pred - goal_pred

```


*Learn*
```{python}
weight = 0.1 
alpha = 0.01 

def neural_network(input, weight):
    prediction = input * weight
    return prediction
  
number_of_toes = [8.5]
win_or_lose_binary = [1] # won 

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input, weight)

error = (pred - goal_pred) ** 2

delta = pred - goal_pred
weight_delta = input * delta


```



Properties: _scaling_ , _negative reversal_, _stopping_



*Updating the weight*

```{python}
weight = 0.1 
alpha = 0.01 

def neural_network(input, weight):
    prediction = input * weight
    return prediction
  
number_of_toes = [8.5]
win_or_lose_binary = [1] # won 

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input, weight)

error = (pred - goal_pred) ** 2

delta = pred - goal_pred
weight_delta = input * delta

weight -= weight_delta * alpha
```

Alpha is the learning rate and controls how fast our Network is learning. 





# Learning is just reducing error

```{python}
weight, goal_pred, input = (0.0, 0.8, 0.5)

for iteration in range(4):
    pred = input * weight 
    error = (pred - goal_pred) ** 2
    delta = pred - goal_pred
    weight_delta = delta * input
    weight = weight - weight_delta
    print("Error:" + str(error) + " Prediction:" + str(pred))
```

There are many ways of measuring error in this case we are using mean squared error. 
Our goal is reducing error to zero. Modify code to make error calculation in one line.



# Several steps of learning 

```{python}
weight, goal_pred, input = (0.0, 0.8, 1.1)

for iteration in range(4) : 
    print ("-----\nWeight:" + str(weight))
    pred = input * weight 
    errror = (pred - goal_pred) ** 2 
    delta = pred - goal_pred
    delta_weight = delta * input
    weight = weight - weight_delta
    print("Error:" + str(error) + " Prediction: " + str(pred))
    print("Delta:" + str(delta) + " Weight Delta:" + str(weight_delta))
```


# Derivatives and how to use them to learn



# Divergence and alpha

* add section headings 
* finish notes summarizing each section

  
  
  
